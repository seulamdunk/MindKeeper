{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e3c5c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade pip\n",
    "#!pip install pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46a8cc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow==2.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e58a0768",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import urllib.request\n",
    "from konlpy.tag import Okt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c918da2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1) 데이터 로드하기\n",
    "\n",
    "train_data = pd.read_table('../ratings_train.txt')\n",
    "test_data = pd.read_table('../ratings_test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f13cdf",
   "metadata": {},
   "source": [
    "# 2) 데이터 정제하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf147d93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(146182, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_data에 중복된 데이터 확인\n",
    "train_data['document'].nunique(), train_data['label'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24256df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "# document 열에서 중복인 내용이 있다면 중복 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c07e488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 리뷰중에서 null값이 있는지 확인\n",
    "train_data.isnull().values.any()\n",
    "\n",
    "# 어떤 열에 null값이 존재하는지 확인\n",
    "train_data.isnull().sum()\n",
    "train_data.loc[train_data.document.isnull()]\n",
    "\n",
    "train_data = train_data.dropna(how = 'any') # Null 값이 존재하는 행 제거\n",
    "train_data.isnull().values.any() # Null 값이 존재하는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1484d6e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'do you expect people to read the FAQ etc and actually accept hard atheism'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 전처리\n",
    "text = 'do!!! you expect... people~ to~ read~ the FAQ, etc. and actually accept hard~! atheism?@@'\n",
    "re.sub(r'[^a-zA-Z ]', '', text) #알파벳과 공백을 제외하고 모두 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "396848d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-6e6a69b985db>:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n"
     ]
    }
   ],
   "source": [
    "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "# 한글과 공백을 제외하고 모두 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efc953a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-47938d712d24>:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  train_data['document'] = train_data['document'].str.replace('^ +', \"\") # white space 데이터를 empty value로 변경\n"
     ]
    }
   ],
   "source": [
    "train_data['document'] = train_data['document'].str.replace('^ +', \"\") # white space 데이터를 empty value로 변경\n",
    "train_data['document'].replace('', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c093ccff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.loc[train_data.document.isnull()][:5]\n",
    "\n",
    "train_data = train_data.dropna(how = 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a5b3e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-21a46579438d>:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  test_data['document'] = test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행\n",
      "<ipython-input-12-21a46579438d>:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  test_data['document'] = test_data['document'].str.replace('^ +', \"\") # 공백은 empty 값으로 변경\n"
     ]
    }
   ],
   "source": [
    "test_data.drop_duplicates(subset = ['document'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거\n",
    "test_data['document'] = test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행\n",
    "test_data['document'] = test_data['document'].str.replace('^ +', \"\") # 공백은 empty 값으로 변경\n",
    "test_data['document'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n",
    "test_data = test_data.dropna(how='any') # Null 값 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bafdc5",
   "metadata": {},
   "source": [
    "# 3) 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82f7b307",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 불용어 제거\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a763ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt()\n",
    "# okt.morphs('와 이런 것도 영화라고 차라리 뮤직비디오를 만드는 게 나을 뻔', stem = True)\n",
    "\n",
    "# stem = True\n",
    "# 일정 수준의 정규화를 수행해준다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc4bf8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "for sentence in train_data['document']:\n",
    "    temp_X = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "    X_train.append(temp_X)\n",
    "    \n",
    "# train_data 형태소 분석기를 사용하여 토큰화\n",
    "# 불용어 제거후 X_train에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e31059de",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "for sentence in test_data['document']:\n",
    "    temp_X = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "    X_test.append(temp_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad4b274",
   "metadata": {},
   "source": [
    "# 4) 정수 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93ad6122",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd9f26bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 3\n",
    "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9848485a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 단어 개수 중 빈도수 2이하인 단어는 제거.\n",
    "# 0번 패딩 토큰을 고려하여 + 1\n",
    "vocab_size = total_cnt - rare_cnt + 1\n",
    "# print('단어 집합의 크기 :',vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c92f26c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(vocab_size) \n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8ec275d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(train_data['label'])\n",
    "y_test = np.array(test_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b67edc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_train = [index for index, sentence in enumerate(X_train) if len(sentence) < 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c47e80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yiseu\\anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "# 빈 샘플들을 제거\n",
    "X_train = np.delete(X_train, drop_train, axis=0)\n",
    "y_train = np.delete(y_train, drop_train, axis=0)\n",
    "#print(len(X_train))\n",
    "#print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6497805a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def below_threshold_len(max_len, nested_list):\n",
    "  cnt = 0\n",
    "  for s in nested_list:\n",
    "    if(len(s) <= max_len):\n",
    "        cnt = cnt + 1\n",
    "  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee86a99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 중 길이가 30 이하인 샘플의 비율: 94.31944999380003\n"
     ]
    }
   ],
   "source": [
    "max_len = 30\n",
    "below_threshold_len(max_len, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2541094e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, maxlen = max_len)\n",
    "X_test = pad_sequences(X_test, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a7786ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4eef125d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "743e3f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fea30830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1936/1936 [==============================] - 61s 31ms/step - loss: 0.3903 - acc: 0.8226 - val_loss: 0.3626 - val_acc: 0.8425\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.84249, saving model to best_model.h5\n",
      "Epoch 2/15\n",
      "1936/1936 [==============================] - 58s 30ms/step - loss: 0.3282 - acc: 0.8571 - val_loss: 0.3371 - val_acc: 0.8543\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.84249 to 0.85427, saving model to best_model.h5\n",
      "Epoch 3/15\n",
      "1936/1936 [==============================] - 60s 31ms/step - loss: 0.3030 - acc: 0.8719 - val_loss: 0.3286 - val_acc: 0.8587\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.85427 to 0.85868, saving model to best_model.h5\n",
      "Epoch 4/15\n",
      "1936/1936 [==============================] - 61s 31ms/step - loss: 0.2839 - acc: 0.8812 - val_loss: 0.3272 - val_acc: 0.8584\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.85868\n",
      "Epoch 5/15\n",
      "1936/1936 [==============================] - 63s 32ms/step - loss: 0.2679 - acc: 0.8904 - val_loss: 0.3290 - val_acc: 0.8609\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.85868 to 0.86088, saving model to best_model.h5\n",
      "Epoch 6/15\n",
      "1936/1936 [==============================] - 64s 33ms/step - loss: 0.2525 - acc: 0.8984 - val_loss: 0.3306 - val_acc: 0.8573\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.86088\n",
      "Epoch 7/15\n",
      "1936/1936 [==============================] - 64s 33ms/step - loss: 0.2367 - acc: 0.9052 - val_loss: 0.3392 - val_acc: 0.8585\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.86088\n",
      "Epoch 8/15\n",
      "1936/1936 [==============================] - 64s 33ms/step - loss: 0.2214 - acc: 0.9125 - val_loss: 0.3657 - val_acc: 0.8536\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.86088\n",
      "Epoch 00008: early stopping\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=60, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5ce4dc10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1527/1527 [==============================] - 9s 6ms/step - loss: 0.3787 - acc: 0.8489\n",
      "\n",
      " 테스트 정확도: 0.8489\n"
     ]
    }
   ],
   "source": [
    "model.save('best_model.h5')\n",
    "\n",
    "loaded_model = load_model('best_model.h5')\n",
    "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
